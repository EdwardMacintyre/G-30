---
title: "Appendix"
author: "Edward Macintyre"
date: "06/03/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Q1

```{r, echo=FALSE}
# Packages and import data
library(tidyverse)
library(ggpubr)
library(GGally)
library(ggplot2)
dat <- read.csv("CW1_data.csv", header=T)
dat <- as_tibble(dat)

# First column the same as the second so remove it
dat <- dat[-1]
```

Initial Checks

```{r}
summary(dat)
```

Appears to be no factor variables given the initial set up of the data.


Investigate NAs in noncitizen variable

```{r}
plot(dat$noncitizen)
```

Not exactly a great range of values since you would expect most of the population to be citizens. Are citizens just as likely to experience HC as non citizens given say both being of a certain race or gender?

Can't delete the 3 NA observations since would lose 3/50 states. Valid reason to exclude the variable from consideration? Is the variable useful anyway given the other variables?

Lean towards just not considering the variable in any models given the variety on show in the other possible variables eg. unemployed, nonwhite, whitepoverty etc.


Pairs and Correlations

```{r}
vars <- colnames(dat)[-6]
vars <- vars[-1]
round(cor(dat[, vars]), 2)
```

```{r}
ggpairs(data=dat[,vars])
```

```{r}
par(mfrow=c(1,2))
plot(dat$trumpvoter, dat$hrate201015)
plot(dat$trumpvoter, dat$hrate2016)
```

Plots appear to show that rate of HC decreased slightly in 2016 compared to 2010-15 average. Correlation confirms this observation. Clear outlier value is Washington DC. Might have to look at excluding this State for the purposes of the model, HC rate wildly different to all other state (not a state anyway).


## Removing Washington DC

```{r}
# Filter out District of Columbia
dat_xDC <- dat %>% filter(state!="District of Columbia")
```

Looking at correlations again

```{r}
round(cor(dat_xDC[, vars]), 2)
```

```{r}
par(mfrow=c(1,2))
plot(dat_xDC$trumpvoter, dat_xDC$hrate201015)
plot(dat_xDC$trumpvoter, dat_xDC$hrate2016)
```

Removing DC lessens the correlation however it is still negative.


```{r}
par(mfrow=c(1,2))
plot(density(dat$hrate201015), ylim=c(0,0.35))
lines(density(dat$hrate2016), col="red")
plot(density(dat_xDC$hrate201015), ylim=c(0,0.35))
lines(density(dat_xDC$hrate2016), col="red")
```

Plots above again don't show any major increases/decreases in the rate of HC in 2016 compared to 2010-15. If anything it shows a slight decrease.


Making a "difference" variable and initial investigation

```{r}
# Creating difference variables for investigation
dat_xDC <- mutate(dat_xDC, incidentsDIFF=incidents2016-incidents201015, popDIFF=pop2016-pop201015, hrateDIFF=hrate2016-hrate201015)
```

```{r}
summary(dat_xDC[c(17,18,19)])
par(mfrow=c(1,2))
plot(density(dat_xDC$incidentsDIFF))
plot(density(dat_xDC$hrateDIFF))
```

"Differences" appear to be centered around zero possible slight skew to left (decrease) on hrateDIFF.


First model - hrateDIFF as response

```{r}
DIFFlm_full <- lm(hrateDIFF ~ householdincome + unemployed + metropop + highschoolpop + whitepoverty + giniindex + nonwhite + trumpvoter, data=dat_xDC)
summary(DIFFlm_full)
```

Linear model since poisson can't take negative values and some differences are obviously negative.

```{r}
summary(DIFFlm_step <- step(DIFFlm_full, trace=0))
```


```{r}
plot(density(residuals(DIFFlm_step)))

par(mfrow=c(1,2))
plot(DIFFlm_step,c(1,2))

par(mfrow=c(2,2))
plot(dat_xDC$householdincome, residuals(DIFFlm_step))
plot(dat_xDC$metropop, residuals(DIFFlm_step))
plot(dat_xDC$trumpvoter, residuals(DIFFlm_step))
```

Doesn't seem to good, poor residual diagnostics, no clear indications of how to improve model. Rsquared low.
Conclusions not clear?

Maybe modelling DIFF as response isn't the appropriate method.




Model 2016 rate as response


Since the response is rate it is not an integer, therefore cannot use poisson despite rate being derived from count. Instead use a Gamma.


```{r}
rateGLM_full <- glm(hrate2016 ~ householdincome + unemployed + metropop + highschoolpop + whitepoverty + giniindex + nonwhite + trumpvoter, data=dat_xDC, family=Gamma(log))
summary(rateGLM_step <- step(rateGLM_full, trace=0))
```


```{r}
plot(density(residuals(rateGLM_step)))

par(mfrow=c(1,2))
plot(rateGLM_step,c(1,2))
```

Residual diagnostics are okay.
Model suggests:
Higher white poverty percentage leads to higher HC rate.
Higher nonwhite populations (percentage) results in lower HC rate.
Higher proportion of trump voters leads to lower HC rate.



Try using incident count as response with population as an offset:

```{r}
incidentGLM_full <- glm(log(incidents2016) ~ householdincome + unemployed + metropop + highschoolpop + whitepoverty + giniindex + nonwhite + trumpvoter, offset=log(pop2016), data=dat_xDC, family=Gamma(log))
summary(incidentGLM_step <- step(incidentGLM_full, trace=0))
```

"Algorithm did not converge" ????




DO WE NEED TO MAKE A MODEL FOR THIS FIRST QUESTION????

SEEMS DATA CLEARLY SHOWS THAT HATE CRIME RATES DECREASED FROM 2010-15 TO 2016, CERTAINLY DIDN'T INCREASE. SO HOW CAN WE APPLY ANY OF THESE CONCLUSIONS TO TRUMP CAMPAIGNS????

ONLY LINK TO TRUMP IN DATA APPEARS TO BE TRUMP VOTER PERCENTAGE (UNLESS WE MAKE SWEEPING ASSUMPTIONS ABOUT CHARACTERISTICS OF TRUMP VOTERS) AND HIGHER PROPORTION OF TRUMP VOTERS APPEAR TO LEAD TO LOWER HATE CRIME RATES - CONCLUSIONS????

MAYBE HAVE THE WRONG APPROACH????



Fitting a linear model with rate as response despite rate being strictly positive

```{r}
rateLM_full <- lm(hrate2016 ~ householdincome + unemployed + metropop + highschoolpop + whitepoverty + giniindex + nonwhite + trumpvoter, data=dat_xDC)
summary(rateLM_step <- step(rateLM_full, trace=0))
```

```{r}
plot(density(residuals(rateLM_step)))

par(mfrow=c(1,2))
plot(rateLM_step,c(1,2))

par(mfrow=c(2,2))
plot(dat_xDC$highschoolpop, residuals(rateLM_step))
plot(dat_xDC$whitepoverty, residuals(rateLM_step))
plot(dat_xDC$trumpvoter, residuals(rateLM_step))
```

Diagnostics seem okay, not great mind. Residual summary not perfect, low Rsquared.

Model suggests:
Higher percentage of population reaching the high school level, the higher hate crime rate.
Higher white poverty rates increase hate crime rate.
Higher trump voter percentage lowers the hate crime rate.

WHO KNOWS WHATS GOING ON

```{r}
AIC(rateLM_step)
AIC(rateGLM_step)
```
AIC prefers Gamma GLM.




WHO KNOWS HOW TO USE ANY OF THESE MODELS TO ANSWER THE QUESTION. QUESTION SEEMS TO GENERAL FOR THE DATA AND VARIABLES GIVEN.
ONLY VARIABLES RELATING TO TRUMP ELECTION IS TIME PERIOD, AND TRUMP VOTER PERCENTAGE.

EVEN WITHOUT MAKING THE MODELS, RATE APPEARS TO DECREASE SLIGHTLY, AND REASONABLE NEGATIVE CORRELATION WITH TRUMP VOTER AND 2016 RATE.

CAN'T CONFIDENTLY CONCLUDE (AT THE MOMENT) THAT RATE INCREASED POST ELECTION, LET ALONE DUE TO THE TRUMP CAMPAIGNS. IF ANYTHING, MODELS AND EXPLANATORY ANALYSIS INDICATE A DECREASE WHICH SEEMS UNUSUAL (GIVEN PRE ASSUMED THOUGHTS ON TRUMP PRESIDENCY).

NO EVIDENCE TO SUPPORT STATEMENT IN QUESTION 1!!!!!!!





LIMITATIONS:

IDEALLY WOULD CONDUCT TIME SERIES ANALYSIS FOR A BETTER UNDERSTANDING OF ANY TREND

SOME HATE CRIMES WILL NOT BE REPORTED LEADING TO INCOMPLETE DATA

WOULD BE GOOD TO HAVE INDIVIDUAL HATE CRIMES SPLIT OUT IE. RACISM, SEXISM ETC ETC TO SEE IF THERE IS A TREND FOR SPECIFIC HATE CRIME, IE RACISM INCREASED BUT SEXISM DECREASED, THAT EFFECT WOULD BE MASKED IN THIS DATA SET.

ONLY 50 STATES (49 ONCE DC REMOVED) SO SMALL DATA SET, MAYBE A LARGER DATA SET COULD SPLIT STATES FURTHER INTO COUNTIES SAY, HOWEVER THIS WOULD THEN BE AFFECTED BY LOW REPORTED NUMBERS, IE WOULD EXPECT MOUNTAINS IN MONTANA TO HAVE VERY FEW REPORTED HATE CRIMES.







STATISTICAL TESTS METHOD


Perform a paired t-test on hrate201015 and hrate2016.

Need to verify that the normality assumption on the differences holds. Use a Shapiro-Wilk test to do this:

```{r}
# Create differences
differences <- dat_xDC$hrate2016-dat_xDC$hrate201015

# Spot checks
length(differences)
differences[1]
differences[33]
dat_xDC$hrate2016[1]-dat_xDC$hrate201015[1]
dat_xDC$hrate2016[33]-dat_xDC$hrate201015[33]
```


Perform normality test:

```{r}
shapiro.test(differences)
```

Shapiro-Wilk test result is SIGNIFICANT, reject the null in favour of the alternative, ie differences are not normally distributed, so paired t-test assumption fails.

We're gonna do it anyway.

```{r}
t.test(dat_xDC$hrate2016, dat_xDC$hrate201015, paired=T, alternative="two.sided")
```

Two sided paired t-test result is NOT SIGNIFICANT, do not reject the null hypothesis, ie mean of differences is zero HOWEVER SINCE THE NORMALITY ASSUMPTION FAILED WE CANNOT CONCLUDE WITH THIS RESULT.



Since normality assumption fails (Shapiro-Wilk), we can try a paired Wilcoxon signed rank test:

Differences must be symmetrically distributed around the median. Check this:

```{r}
median(differences)
IQR(differences)
```

```{r}
boxplot(differences)
```

Differences do appear to be symmetrically distributed around the median, assumption holds, do the test.


```{r}
wilcox.test(dat_xDC$hrate2016, dat_xDC$hrate201015, paired=T, alternative="two.sided")
```

Two sided paired Wilcoxon signed rank test result is NOT SIGNIFICANT, we do not rejct the null hypothesis, ie median of the differences is zero and we can conclude that there is no statistically significant change in the hate crime rate between 201015 and 2016.


In conclusion, over the years 2010-15 and 2016 there was no change in the hate crime rate. Therefore, when taking hate crime in its most general form (ie. not split up into racism, sexism etc.), there is no evidence in our data to suggest that the presidential election of Donald Trump in 2016 did not lead to an increase in hate crime rate across the USA.


```{r}
mean(dat_xDC$hrate201015)
mean(dat_xDC$hrate2016)

median(dat_xDC$hrate201015)
median(dat_xDC$hrate2016)

var(dat_xDC$hrate201015)
var(dat_xDC$hrate2016)
```

```{r}
mean(differences)
median(differences)
var(differences)
```







Q2

```{r, echo=FALSE}
library(MASS)
library(tidyverse)
library(ggpubr)
library(GGally)
library(Hmisc)
library(corrplot)
library(lmtest)
library(boot)
library(DAAG)
library(ggthemes)
library(gridExtra)
library(gt)
library(ggiraphExtra)
library(sjPlot)


data_raw <- read_csv("CW1_data.csv")

# ++++++++++++++++++++++++++++
# flattenCorrMatrix
# ++++++++++++++++++++++++++++
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}

```
We can drop the first column since it is identical to the state variable
```{r}
data_raw <- data_raw[-1]
```
Keep only the variables of interest: those being the socioeconomic variables from or close to the 2010-2015 period, and the hate crime incidents, rates and the population covered from the 2010-2015 
```{r}
# Select the variables we require
data201015 <- data_raw %>% select(-c("incidents2016", "pop2016", "hrate2016"))
summary(data201015)
```
We wish to see if there is an association between hate crime incidence rates and these socioeconomic variables. To do this, we will need to fit a GLM.

Since the response of hate crime incidence rate is a positive rate, it is natural to try to fit a Gamma GLM. Also, since the rate is a mean across a 5 year period, due to the central limit theorem, the distribution of the variable would tend towards a normal, hence a standard linear model may also be appropriate. Furthermore, since hate crime incidents is a count variable, it could be feasible to fit a Negative Binomial model with log(population/100000) as an offset variable. Let's fit all three models, refine each of them, and compare their predictive capabilities.

Notice that "noncitizen" has three missing values. We will need to decide whether to include this variable in our model and exclude these missing rows, or exclude the variable and include the entirety of the remaining data.

Since the incidence rates are directly related to the number of incidents via division by the population and multiplication by 100,000, it would not make sense to include incidence as an explanatory variable. However it could be feasible for population size to affect the overall hate crime rate, therefore it will be included in the initial modelling.

Here are visualisations of the mean hate crime rate from 2010-2015
```{r}
box <- ggplot(data201015, aes(x = hrate201015, y = "")) + geom_boxplot(fill = "grey", outlier.color = "red", outlier.size = 3) + labs(title = "Mean annual hate crime incidents per 100,000 (2010-2015)", x=NULL,y=NULL) + theme_wsj()+ theme(text = element_text(size = 5), plot.title = element_text(hjust = 0.5), axis.text = element_text(size = 10))
hist <- ggplot(data201015, aes(x = hrate201015)) + geom_histogram(bins=40, fill = "grey", colour = "black") + labs(x="Rate",y=NULL) + theme_wsj() + theme(text = element_text(size = 5), plot.title = element_text(hjust = 0.5), axis.text = element_text(size = 10))
grid.arrange(box,hist,nrow=2)
```
We can clearly see that there is a outlier of 10.97, corresponding to the hate crime rate of the District of Columbia. It is likely that this value is extreme due to the fact that the District of Columbia is a much smaller, more densely populated metropolitan area. We will remove this row.
```{r}
data201015 <- data201015 %>% filter(state != "District of Columbia")
```
Here is a visualisation of the correlations between all of the variables
```{r}
corrplot(cor(data201015[-1], use = "complete.obs"), type = "full", order = "hclust", tl.col = "black", tl.srt = 45, bg = "#f8f2e4", method = "color")
```
Here is a table of the correlations and their p-values
```{r}
cor_mat <- rcorr(as.matrix(data201015[-1]))
cor_mat_flat <- flattenCorrMatrix(cor_mat$r, cor_mat$P) %>% arrange(desc(cor)) %>% rename(`1`=row,`2`=column, Correlation = cor, `P-Value` = p)
cor_mat_flat
cor_mat_flat_high <- cor_mat_flat %>% filter(abs(Correlation) >= 0.7)
```
We will need to be mindful of the variables that are highly correlated. 

Let's begin with a full linear model
```{r}
lmod_1 <- glm(hrate201015 ~ householdincome + unemployed + metropop + highschoolpop + noncitizen + whitepoverty + giniindex + nonwhite + trumpvoter + pop201015, data = data201015)
summary(lmod_1)
```
The only significant variable here is noncitizen. Let's attempt to remove some variables from the model using the step function
```{r}
step(lmod_1)
```
Set a new model equal to the result of the step function
```{r}
lmod_2 <- glm(formula = hrate201015 ~ highschoolpop + noncitizen + whitepoverty + giniindex + nonwhite + pop201015, data = data201015)
summary(lmod_2)
```
This new model only has two significant variables: noncitizen and nonwhite.
Let's perform a log-likelihood ratio test on whether a model only containing noncitizen and nonwhite is as good as the larger model
```{r}
lmod_3 <- glm(formula = hrate201015 ~ noncitizen + nonwhite, data = data201015)
lrtest(lmod_2, lmod_3)
```
The p-value of the test is not significant, so we do not reject the null hypothesis that this model is as good a fit as the larger model. So we choose to keep only noncitizen and nonwhite.
Let's see which variables contribute the most to the model
```{r}
drop1(lmod_3)
```
Since nonwhite and noncitizen have a strong positive correlation of 0.74, we could try to remove one of them. nonwhite contributes slightly more than noncitizen, so let's remove noncitizen and perform another log-likelihood ratio test. In order to compare the two models, this new model will need to be constructed on the same data set, so let's remove the rows where noncitizen has empty values.
```{r}
data201015_1 <- data201015 %>% filter(!is.na(noncitizen))
lmod_4 <- glm(formula = hrate201015 ~ nonwhite, data = data201015_1)
lrtest(lmod_3, lmod_4)
```
The p-value of this test is highly significant, so we reject the null hypothesis that the smaller model is as good a fit as the larger model.
```{r}
summary(lmod_3)
```
Under this model, both explanatory variables are significant

Let's investigate how well the model fits the data and our normality assumptions
```{r}
plot(lmod_3,1)
plot(lmod_3,2)
```
The Q-Q plot indicates a fairly good normality assumption. There are some outliers, however, since the data set is already small and the outliers aren't too extreme, they will not be removed. The residual vs fitted plot indicates good centrality about zero, however there appears to be non-constant variance as the fitted value increases, which is undesirable.
```{r}
plot(data201015_1$noncitizen, resid(lmod_3))
lines(lowess(data201015_1$noncitizen,resid(lmod_3)),col="red")
abline(h=0,col="blue")

plot(data201015_1$nonwhite, resid(lmod_3))
lines(lowess(data201015_1$nonwhite,resid(lmod_3)),col="red")
abline(h=0,col="blue")
```
The individual residual plots for noncitizen and nonwhite indicate good normality assumptions: no visible patterns and an even distribution about zero



Now let's try to fit a gamma model.
```{r}
gmod_1 <- glm(hrate201015 ~ householdincome + unemployed + metropop + highschoolpop + noncitizen + whitepoverty + giniindex + nonwhite + trumpvoter + pop201015, data = data201015, family = Gamma("log"))
summary(gmod_1)
```
The only significant variables are noncitizen and nonwhite. Let's attempt to remove some variables from the model using the step function
```{r}
step(gmod_1)
```
Set a new model equal to the result of the step function
```{r}
gmod_2 <- glm(formula = hrate201015 ~ noncitizen + nonwhite, family = Gamma("log"), data = data201015)
summary(gmod_2)
```
This new model only contains two variables: noncitizen and nonwhite. Both are significant. However, they have a strong positive corellation, so let's attempt to remove one.
```{r}
drop1(gmod_2)
```
nonwhite contributes the most to the model, so we remove noncitizen and perform a log-likelihood ratio test.
```{r}
gmod_3 <- glm(formula = hrate201015 ~ nonwhite, family = Gamma("log"), data = data201015_1)
lrtest(gmod_2, gmod_3)
```
The p-value of this test is highly significant, so we reject the null hypothesis that the smaller model is as good a fit as the larger model.

Let's investigate how well the model fits the data and our assumptions
```{r}
gmod_2_graph_data <- data.frame(Residuals = resid(gmod_2,type="deviance"), Predicted = predict(gmod_2,type="link"))
ggplot(gmod_2_graph_data, aes(Predicted,Residuals)) + geom_point() + geom_abline(intercept = 0, slope = 0) + stat_smooth(se=FALSE)

```
The residual plot is similar to that of the linear model (lmod_3): there appears to be a larger variance of the residuals as the linear predictor increases but a fairly even proportion of over/under predicted values. 





Now let's fit a negative binomial model.
```{r}
nbmod_1 <- glm.nb(incidents201015 ~ householdincome + unemployed + metropop + highschoolpop + noncitizen + whitepoverty + giniindex + nonwhite + trumpvoter + offset(log(pop201015/100000)), data = data201015)
summary(nbmod_1)
```
Only noncitizen and nonwhite are significant. Let's try to reduce the number of variables in the model using the step function and set a new model equal to the result.
```{r}
step(nbmod_1)
nbmod_2 <- glm.nb(formula = incidents201015 ~ noncitizen + nonwhite + offset(log(pop201015/1e+05)), data = data201015, link = log)
summary(nbmod_2)
```
Again, since noncitizen and nonwhite have a strong positive correlation, let's evaluate their contributions to the likelihood and see if we can remove the one with the least contribution.
```{r}
drop1(nbmod_2)
```
Remove noncitizen and perform a log-likelihood ratio test to see if the smaller model is as good a fit as the larger model
```{r}
nbmod_3 <- glm.nb(formula = incidents201015 ~ nonwhite + offset(log(pop201015/1e+05)), data = filter(data201015, !is.na(noncitizen)), link = log)
lrtest(nbmod_2,nbmod_3)
```
The test is highly statistically significant, as for the linear and gamma models, so we opt to keep noncitizen.

Now let's have a look at the spread of the deviance residuals against the linear predictor.
```{r}
nbmod_2_graph_data <- data.frame(Residuals = resid(nbmod_2,type="deviance"), Predicted = predict(nbmod_2,type="link"))
ggplot(nbmod_2_graph_data, aes(Predicted,Residuals)) + geom_point(size = 2) + geom_abline(intercept = 0, slope = 0) + stat_smooth(se=FALSE,color="red") + labs(title = "Residual plot for Negative Binomial model", x="Linear Predictor",y="Deviance Residual") + theme_wsj() + theme(text = element_text(size = 5), plot.title = element_text(hjust = 0.5), axis.text = element_text(size = 10), axis.title = element_text()) + scale_y_continuous(limits = c(-3,3), n.breaks = 6)
```
This plot is very similar to that of the Gamma model.

Now we wish to decide which of the three models are the best. They all only have noncitizen and nonwhite as explanatory variables, and none of their residual plots are particularly good. Whereas normally we would use AIC to compare different models, we cannot use it here to compare the linear, gamma and negative binomial models, since they have different likelihood functions.

Instead, lets perform leave-one-out cross validation to compare their predictive powers.


```{r}
sum <- 0
for (i in 1:nrow(data201015_1)) {
  data_temp <- data201015_1[-i,]
  mod_temp <- glm(formula(lmod_3), data = data_temp)
  terms <- data201015_1[i,c("noncitizen","nonwhite")]
  predicted <- predict.glm(mod_temp, newdata = terms, type = "response")
  actual <- data201015_1[i,c("hrate201015")]
  square_diff <- (predicted - actual)^2
  sum <- sum + square_diff
}
average_error_l <- sum/nrow(data201015_1)

sum <- 0
for (i in 1:nrow(data201015_1)) {
  data_temp <- data201015_1[-i,]
  mod_temp <- glm(formula(gmod_2), data = data_temp, family = Gamma("log"))
  terms <- data201015_1[i,c("noncitizen","nonwhite","pop201015")]
  predicted <- predict.glm(mod_temp, newdata = terms, type = "response")
  actual <- data201015_1[i,c("hrate201015")]
  square_diff <- (predicted - actual)^2
  sum <- sum + square_diff
}
average_error_g <- sum/nrow(data201015_1)


sum <- 0
for (i in 1:nrow(data201015_1)) {
  data_temp <- data201015_1[-i,]
  mod_temp <- glm.nb(formula(nbmod_2), data = data_temp)
  terms <- data201015_1[i,c("noncitizen","nonwhite","pop201015")]
  predicted <- predict.glm(mod_temp, newdata = terms, type = "response")/(terms$pop201015/1e+05)
  actual <- data201015_1[i,c("hrate201015")]
  square_diff <- (predicted - actual)^2
  sum <- sum + square_diff
}
average_error_nb <- sum/nrow(data201015_1)
average_error_nb
```
The Negative Binomial model produced the lowest prediction error, meaning it is the most versatile and making predictions to new data. For this reason, this is the best chosen model.






```{r}
nbmod_2_graph_data_1 <- data.frame(Actual = data201015_1$hrate201015, Predicted = predict(nbmod_2, type = "response")/(data201015_1$pop201015/100000))
ggplot(nbmod_2_graph_data_1, aes(Predicted,Actual)) + geom_point(size = 2) + geom_abline(intercept = 0, slope = 1) + stat_smooth(se=FALSE,color="red") + labs(title = "Actual vs Predicted hate crime rates for NB model", x="Predicted",y="Actual") + theme_wsj() + theme(text = element_text(size = 5), plot.title = element_text(hjust = 0.5), axis.text = element_text(size = 10), axis.title = element_text())
```
```{r}
confint(nbmod_2)
```
Parameters:

noncitizen:   13.58 (5.59,21.62) -> For every 1% increase in noncitizen, hate crime rate increases by 1.15 (1.06,1.24)
nonwhite      -3.10 (-4.84,-1.31) -> For every 1% increase in nonwhite, hate crime rate decreases by 0.0305 (0.0130,0.0473)

According to the model, a fully white, fully US citizen state would have a hate crime rate of 2.58 (1.73,3.89)


Caveats

1. The response is an average across the 2010-2015 period, whereas the explanatory variables are taken from singular years within or close to the 2010-2015 period.

2. The maximum value of noncitizen is 0.130, so attempting to make predictions with non-citizen proportions greater than this value are likely to be inaccurate. Similarly for nonwhite with 0.630

3. The data set is relatively small and very noisy. This means it is difficult to create a model that fits the data well.

References

1. flattenCorrMatrix function, "Correlation matrix : A quick start guide to analyze, format and visualize a correlation matrix using R software", from "http://www.sthda.com/english/wiki/correlation-matrix-a-quick-start-guide-to-analyze-format-and-visualize-a-correlation-matrix-using-r-software#infos", on 02/03/22 
